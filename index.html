<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning</title>
  <link rel="icon" type="image/x-icon" href="static/images/efficiency.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered" style="margin-bottom: 1.5rem;">
      Token-level Consistency and Speedup Analysis
    </h2>

    <!-- Intro text -->
    <div class="content has-text-justified" style="font-size: 1rem;">
      <p>
        Speculative decoding has received considerable attention due to its potential for substantial speedups. However, its effectiveness critically depends on the <b>consistency between the small language model (SLM) and the large language model (LLM)</b>. We quantify this limitation using <i>token-level consistency</i>, defined as the percentage of tokens for which the SLM produces the same output as the LLM given an identical prefix.
      </p>
      <p>
        Figure&nbsp;1 illustrates three aspects: (a) consistency–speedup trade-offs across different model pairs, (b) distribution of speedups across AMC samples, and (c) token length comparison for correctly answered questions. The results highlight that speculative decoding may suffer overheads when agreement is low, and fails to exploit the concise reasoning of SLMs effectively.
      </p>
    </div>

    <!-- Coverflow carousel -->
    <div class="coverflow-wrapper" style="position: relative; width: 100%; max-width: 900px; margin: 2rem auto; overflow: hidden;">
      <div class="coverflow" style="display: flex; justify-content: center; align-items: center; position: relative;">

        <!-- Slide 1 -->
        <div class="coverflow-item">
          <img src="static/images/motivation_consistency_with_upperbound.png" alt="Token-level consistency vs. speedup">
          <p><b>(a)</b> Token-level consistency vs. speedup across LLMs.</p>
        </div>

        <!-- Slide 2 -->
        <div class="coverflow-item">
          <img src="static/images/speedup_distribution_filtered.png" alt="Speedup distribution">
          <p><b>(b)</b> Speedup distribution across AMC samples.</p>
        </div>

        <!-- Slide 3 -->
        <div class="coverflow-item">
          <img src="static/images/token_length_bars.png" alt="Token usage comparison">
          <p><b>(c)</b> Token usage by SLM vs. LLM on correct answers.</p>
        </div>

      </div>

      <!-- Arrows -->
      <button onclick="prevCover()" style="position: absolute; top: 50%; left: 10px; transform: translateY(-50%); background: rgba(255,255,255,0.9); border: none; border-radius: 50%; width: 36px; height: 36px; cursor: pointer; box-shadow: 0 2px 6px rgba(0,0,0,0.2);">◀</button>
      <button onclick="nextCover()" style="position: absolute; top: 50%; right: 10px; transform: translateY(-50%); background: rgba(255,255,255,0.9); border: none; border-radius: 50%; width: 36px; height: 36px; cursor: pointer; box-shadow: 0 2px 6px rgba(0,0,0,0.2);">▶</button>
    </div>

    <!-- Caption -->
    <div class="has-text-centered" style="margin-top: 1.5rem; font-size: 0.95rem; color: #555;">
      <b>Figure 1.</b> Token-level consistency and speedup analysis.  
      (a) Consistency vs. speedup across model pairs.  
      (b) Distribution of speedup ratios across AMC samples.  
      (c) Token usage comparison between SLM and LLM on correct answers.
    </div>
  </div>
</section>

<style>
  .coverflow-item {
    position: absolute;
    transition: all 0.6s ease;
    text-align: center;
  }
  .coverflow-item img {
    max-width: 70%;
    border-radius: 12px;
    box-shadow: 0 4px 12px rgba(0,0,0,0.15);
  }
  .coverflow-item p {
    margin-top: 0.5rem;
    font-size: 0.9rem;
  }
</style>

<script>
  const items = document.querySelectorAll('.coverflow-item');
  let current = 0;

  function updateCoverflow() {
    items.forEach((item, i) => {
      item.style.opacity = "0";
      item.style.zIndex = "0";
      item.style.transform = "translateX(0) scale(0.7)";
    });

    const total = items.length;
    const left = (current - 1 + total) % total;
    const right = (current + 1) % total;

    // 中间
    items[current].style.opacity = "1";
    items[current].style.zIndex = "3";
    items[current].style.transform = "translateX(0) scale(1)";

    // 左边
    items[left].style.opacity = "0.5";
    items[left].style.zIndex = "2";
    items[left].style.transform = "translateX(-120%) scale(0.85)";

    // 右边
    items[right].style.opacity = "0.5";
    items[right].style.zIndex = "2";
    items[right].style.transform = "translateX(120%) scale(0.85)";
  }

  function prevCover() {
    current = (current - 1 + items.length) % items.length;
    updateCoverflow();
  }

  function nextCover() {
    current = (current + 1) % items.length;
    updateCoverflow();
  }

  // 初始化
  updateCoverflow();
</script>



<!-- Video carousel -->
<section class="hero is-small is-light" >
  <div class="hero-body">
    <div class="container" style="max-width: 70%; margin: 0 auto;">
      <h2 class="title is-3">Methodology</h2>
      <div class="has-text-centered">
      <figure class="image is-inline-block">
        <img src="static/images/cot_methodv2.png" alt="Method illustration"  style="width: 95%;">
      </figure>
      <figcaption>
        <b>Overview of R-Stitch.</b> Given a question with chain-of-thought (CoT) prompting, decoding alternates between a small language model (SLM) and a large language model (LLM) based on token-level confidence. Generation begins with the SLM. If the predicted token has low confidence, the system switches to the LLM, which overwrites the uncertain token and continues decoding. Conversely, when the LLM produces a high-confidence token, control is handed back to the SLM to reduce computational cost. This bidirectional switching mechanism enables dynamic resource allocation based on confidence, allowing R-Stitch~to retain the efficiency of the SLM while leveraging the reliability of the LLM when necessary.
      </figcaption>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="max-width: 70%; margin: 0 auto;">
      <h2 class="title is-3">Performance</h2>

      <div class="has-text-centered">
        <figure class="image is-inline-block">
          <img src="static/images/performance.png" alt="Performance">
        </figure>
      </div>

    </div>
  </div>
</section>


<!--
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container content" style="max-width: 70%; margin: 0 auto;">
      <h2 class="title is-3">Limitations and Future Work</h2>

      <p>
        The current routing policy is based on a fixed confidence threshold, which may lead to unstable performance across diverse input distributions and task types. In particular, confidence scores may not always accurately reflect token difficulty, resulting in suboptimal routing decisions. Additionally, our implementation <strong>currently supports only batch size 1</strong> due to dynamic token-level model switching, which limits practical deployment and hardware utilization. Addressing this limitation may require designing new scheduling strategies or restructuring the routing mechanism to better accommodate batched inference.
      </p>

      <p>
        To further alleviate the KV Cache and parameter burden from the two models, we plan to adopt <em>parameter-sharing strategies</em> such as mixture-of-depth/width to enhance memory efficiency. As a future direction, we also plan to explore <strong>reinforcement learning-based policies</strong> that can adaptively switch models based on feedback signals like downstream task performance or decoding stability. These extensions could improve the robustness and scalability of dynamic model routing, and enable more fine-grained control over the latency-accuracy trade-off.
      </p>
    </div>
  </div>
</section>
-->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{chen2025r,
        title={R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning},
        author={Chen, Zhuokun and Chen, Zeren and He, Jiahao and Tan, Mingkui and Cai, Jianfei and Zhuang, Bohan},
        journal={arXiv preprint arXiv:2507.17307},
        year={2025}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
