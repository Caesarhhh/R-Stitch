<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning</title>
  <link rel="icon" type="image/x-icon" href="static/images/efficiency.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>





<!-- Image carousel -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="mailto:caesard216@gmail.com" target="_blank">Zhuokun Chen</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Zeren Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Jiahao He</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Lu Sheng</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Mingkui Tan</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Jianfei Cai</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="mailto:bohan.zhuang@gmail.com" target="_blank">Bohan Zhuang</a><sup>4†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup> Monash University &nbsp;&nbsp;
              <sup>2</sup> School of Software, Beihang University &nbsp;&nbsp;
              <sup>3</sup> South China University of Technology &nbsp;&nbsp;
              <sup>4</sup> ZIP Lab, Zhejiang University
            </span>
            <span class="eql-cntrb"><small>Email: caesard216@gmail.com, bohan.zhuang@gmail.com</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2507.17307" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/Caesarhhh/R_Stitch" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              
              <span class="link-block">
                <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:L-aXZ6FU0VYJ:scholar.google.com/&output=citation&scisdr=CgIBXavQENivsT6gox0:AAZF9b8AAAAAaIWmux1fNZ6UnwDxaQkLs1214k4&scisig=AAZF9b8AAAAAaIWmuxkLjAKSxtpVwuQin0mfxMw&scisf=4&ct=citation&cd=-1&hl=zh-CN" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span>BibTex</span>
                </a>
              </span>
              <!-- ArXiv abstract Link -->
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<div class="content has-text-centered" style="font-size: 1.1rem;">
  <p>
    <strong>News:</strong><br>
    <span style="font-weight: bold;">(07/24/2025)</span> 🎉 First version of the project is released on 
    <a href="https://arxiv.org/abs/2507.17307" target="_blank">arXiv</a>.
  </p>
</div>



<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Chain-of-thought (CoT) enhances the problem-solving ability of large language models (LLMs) but incurs substantial inference cost due to long autoregressive trajectories. Existing acceleration strategies either shorten traces via early stopping or compression, or adopt speculative decoding with a smaller model. However, speculative decoding provides limited gains when model agreement is low and rigidly enforces token-level consistency, overlooking the observation that some smaller models, when correct, produce significantly more concise reasoning traces that could reduce inference length. We introduce <em>R-Stitch</em>, a training-free hybrid decoding framework that leverages token-level entropy as an uncertainty proxy to delegate computation between a small language model (SLM) and an LLM. Our analysis shows that high-entropy tokens are more likely to induce errors, motivating an entropy-guided routing strategy that lets the SLM efficiently handle low-entropy tokens while delegating uncertain ones to the LLM, thereby avoiding full rollbacks and preserving answer quality. We further extend this design with <em>R-Stitch<sup>+</sup></em>, which learns an adaptive routing policy to adjust the token budget dynamically beyond fixed thresholds. By jointly reducing per-token decoding complexity and the number of generated tokens, our method achieves substantial acceleration with negligible accuracy loss. Concretely, it attains peak speedups of <b>3.00×</b> on DeepSeek-R1-Distill-Qwen-7B, <b>3.85×</b> on 14B, and <b>4.10×</b> on QWQ-32B while maintaining accuracy comparable to full LLM decoding. Moreover, it naturally enables adaptive efficiency–accuracy trade-offs that can be tailored to diverse computational budgets without retraining. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered" 
        style="margin-bottom: 1.5rem; border: 2px solid #ddd; padding: 0.5rem 1rem; border-radius: 8px; display:inline-block;">
      Token-level Consistency and Speedup Analysis
    </h2>

    <!-- 分析文字 -->
    <div class="content has-text-justified" style="font-size: 1rem; margin-bottom: 1.5rem;">
      <p>
        Speculative decoding has received considerable attention due to its potential for substantial speedups. 
        However, its effectiveness critically depends on the <b>consistency between the small language model (SLM) 
        and the large language model (LLM)</b>. We quantify this limitation using <i>token-level consistency</i>, 
        defined as the percentage of tokens for which the SLM produces the same output as the LLM given an identical prefix.
      </p>
      <p>
        Figure&nbsp;1 illustrates three aspects: (a) consistency–speedup trade-offs across different model pairs, 
        (b) distribution of speedups across AMC samples, and (c) token length comparison for correctly answered questions. 
        The results highlight that speculative decoding may suffer overheads when agreement is low, 
        and fails to exploit the concise reasoning of SLMs effectively.
      </p>
    </div>

    <!-- Slider -->
    <div class="slider-token-wrapper" style="position: relative; height: 480px; overflow: visible; margin-top: 2rem;">
      <div class="slide-token">
        <img src="static/images/motivation_consistency_with_upperbound.png" 
             alt="Token-level consistency vs. speedup">
        <p><b>(a)</b> Token-level consistency vs. speedup.</p>
      </div>
      <div class="slide-token">
        <img src="static/images/speedup_distribution_filtered.png" 
             alt="Speedup distribution">
        <p><b>(b)</b> Speedup distribution.</p>
      </div>
      <div class="slide-token">
        <img src="static/images/token_length_bars.png" 
             alt="Token usage comparison">
        <p><b>(c)</b> Token usage by SLM vs. LLM.</p>
      </div>

      <!-- Nav -->
      <button onclick="prevSlideToken()">◀</button>
      <button onclick="nextSlideToken()">▶</button>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered" 
        style="margin-bottom: 1.5rem; border: 2px solid #ddd; padding: 0.5rem 1rem; border-radius: 8px; display:inline-block;">
      Empirical Entropy Analysis
    </h2>

    <!-- 精简文字 -->
    <div class="content has-text-justified" style="font-size: 1rem; margin-bottom: 1.5rem;">
      <p>
        We analyze entropy patterns on AMC using DeepSeek-R1-Distill-Qwen-7B (LLM) and L1-1.5B-Short (SLM), 
        revealing three key observations:
      </p>
      <p><b>1. Incorrect answers show higher entropy.</b>  
        Reasoning traces leading to wrong answers have significantly higher mean entropy (Figure&nbsp;2a).
      </p>
      <p><b>2. Most tokens are near-deterministic.</b>  
        Over 89% of SLM tokens have entropy ≤ 0.1, indicating high prediction confidence (Figure&nbsp;2b).
      </p>
      <p><b>3. High-entropy tokens trigger errors.</b>  
        Harmful tokens are often preceded by locally elevated entropy, making entropy a useful routing signal (Figure&nbsp;2c).
      </p>
    </div>

    <!-- Slider -->
    <div class="slider-entropy-wrapper" style="position: relative; height: 460px; overflow: visible; margin-top: 2rem;">
      <div class="slide-entropy">
        <img src="static/images/motivation/entropy_amc.png" alt="Entropy AMC">
        <p><b>(a)</b> Higher entropy in incorrect solutions.</p>
      </div>
      <div class="slide-entropy">
        <img src="static/images/motivation/token_entropys.png" alt="Token entropy distribution">
        <p><b>(b)</b> Most tokens are near-zero entropy.</p>
      </div>
      <div class="slide-entropy">
        <img src="static/images/motivation/entropy_neighborhood.png" alt="Entropy neighborhood">
        <p><b>(c)</b> Elevated entropy before harmful tokens.</p>
      </div>

      <!-- Nav -->
      <button onclick="prevSlideEntropy()">◀</button>
      <button onclick="nextSlideEntropy()">▶</button>
    </div>

    <!-- Figure caption -->
    <div class="has-text-centered" style="margin-top: 1.5rem; font-size: 0.95rem; color: #555;">
      <b>Figure 2.</b> Entropy and error locality.  
      (a) Incorrect solutions show higher entropy.  
      (b) Most tokens have near-zero entropy.  
      (c) Harmful tokens are preceded by high-entropy regions.
    </div>
  </div>
</section>




<style>
.slide-token, .slide-entropy {
  position: absolute;
  top: 50%; left: 50%;
  width: 60%;
  transform: translate(-50%, -50%) scale(0.7);
  opacity: 0; 
  transition: all 0.6s ease;
  text-align: center;
}

.slide-token img, .slide-entropy img {
  max-width: 80%;   /* ✅ 保持合适大小 */
  border-radius: 12px;
  box-shadow: 0 4px 12px rgba(0,0,0,0.15);
  display: block;
  margin: 0 auto;
}

.slide-token p, .slide-entropy p {
  position: absolute;
  bottom: -2.5rem;  /* 往下移出一些空间 */
  left: 50%;
  transform: translateX(-50%);
  font-size: 0.9rem;
  background: rgba(255,255,255,0.85);
  padding: 0.2rem 1rem;   /* ✅ padding 调大一点 */
  border-radius: 6px;
  display: inline-block;
  white-space: nowrap;    /* ✅ 不允许自动换行 */
  min-width: 280px;       /* ✅ 最小宽度，避免太窄 */
  text-align: center;     /* ✅ 居中文字 */
}


/* 三个状态：只改这里 */
/* 中间图 */
/* 包裹容器，确保外边部分被裁掉 */
.slider-token-wrapper, .slider-entropy-wrapper {
  position: relative;
  height: 420px;
  max-width: 100%;
  overflow: hidden;   /* ✅ 外边部分不显示 */
  margin: 2rem auto;
  isolation: isolate; 
  border-style:solid;
  border-width:5px;
  contain: paint;     /* ✅ 关键：强制建立裁剪上下文 */
}

/* 中间图 */
.active {
  opacity: 1 !important;
  transform: translate(-50%, -50%) scale(1) !important; 
  z-index: 3;
}

/* 左边图：挤到容器左边，只露右半部分 */
.left {
  opacity: 0.6 !important;
  transform: translate(-125%, -50%) scale(0.85) !important;  
  z-index: 2;
}

/* 右边图：挤到容器右边，只露左半部分 */
.right {
  opacity: 0.6 !important;
  transform: translate(25%, -50%) scale(0.85) !important;   
  z-index: 2;
}



/* 按钮修复点击穿透 */
.slider-token-wrapper button,
.slider-entropy-wrapper button {
  position: absolute;
  top: 50%;
  transform: translateY(-50%);
  background: rgba(255,255,255,0.85);
  border: none;
  border-radius: 50%;
  width: 36px;
  height: 36px;
  font-size: 18px;
  cursor: pointer;
  box-shadow: 0 2px 6px rgba(0,0,0,0.2);
  z-index: 10;
  transition: background 0.3s ease;
}
.slider-token-wrapper button:hover,
.slider-entropy-wrapper button:hover {
  background: rgba(255,255,255,1);
}
.slider-token-wrapper button:first-of-type,
.slider-entropy-wrapper button:first-of-type {
  left: 10px;
}
.slider-token-wrapper button:last-of-type,
.slider-entropy-wrapper button:last-of-type {
  right: 10px;
}


</style>


<script>
  function initSlider(wrapperSelector, slideSelector, prevFnName, nextFnName) {
    let currentIndex = 0;
    const slides = document.querySelectorAll(slideSelector);
    if (!slides.length) return;

    function updateSlides() {
      slides.forEach((s,i)=>{
        s.classList.remove("active","left","right");
        if(i===currentIndex) s.classList.add("active");
        else if(i===(currentIndex-1+slides.length)%slides.length) s.classList.add("left");
        else if(i===(currentIndex+1)%slides.length) s.classList.add("right");
      });
    }

    window[prevFnName] = ()=>{ currentIndex=(currentIndex-1+slides.length)%slides.length; updateSlides(); }
    window[nextFnName] = ()=>{ currentIndex=(currentIndex+1)%slides.length; updateSlides(); }

    updateSlides(); // 初始化
  }

  window.addEventListener("load", ()=>{
    initSlider(".slider-token-wrapper",".slide-token","prevSlideToken","nextSlideToken");
    initSlider(".slider-entropy-wrapper",".slide-entropy","prevSlideEntropy","nextSlideEntropy");
  });
</script>


<!-- Video carousel -->
<section class="hero is-small is-light" >
  <div class="hero-body">
    <div class="container" style="max-width: 70%; margin: 0 auto;">
      <h2 class="title is-3">Methodology</h2>
      <div class="has-text-centered">
      <figure class="image is-inline-block">
        <img src="static/images/cot_methodv2.png" alt="Method illustration"  style="width: 95%;">
      </figure>
      <figcaption>
        <b>Overview of R-Stitch.</b> Given a question with chain-of-thought (CoT) prompting, decoding alternates between a small language model (SLM) and a large language model (LLM) based on token-level confidence. Generation begins with the SLM. If the predicted token has low confidence, the system switches to the LLM, which overwrites the uncertain token and continues decoding. Conversely, when the LLM produces a high-confidence token, control is handed back to the SLM to reduce computational cost. This bidirectional switching mechanism enables dynamic resource allocation based on confidence, allowing R-Stitch~to retain the efficiency of the SLM while leveraging the reliability of the LLM when necessary.
      </figcaption>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="max-width: 70%; margin: 0 auto;">
      <h2 class="title is-3">Performance</h2>

      <div class="has-text-centered">
        <figure class="image is-inline-block">
          <img src="static/images/performance.png" alt="Performance">
        </figure>
      </div>

    </div>
  </div>
</section>


<!--
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container content" style="max-width: 70%; margin: 0 auto;">
      <h2 class="title is-3">Limitations and Future Work</h2>

      <p>
        The current routing policy is based on a fixed confidence threshold, which may lead to unstable performance across diverse input distributions and task types. In particular, confidence scores may not always accurately reflect token difficulty, resulting in suboptimal routing decisions. Additionally, our implementation <strong>currently supports only batch size 1</strong> due to dynamic token-level model switching, which limits practical deployment and hardware utilization. Addressing this limitation may require designing new scheduling strategies or restructuring the routing mechanism to better accommodate batched inference.
      </p>

      <p>
        To further alleviate the KV Cache and parameter burden from the two models, we plan to adopt <em>parameter-sharing strategies</em> such as mixture-of-depth/width to enhance memory efficiency. As a future direction, we also plan to explore <strong>reinforcement learning-based policies</strong> that can adaptively switch models based on feedback signals like downstream task performance or decoding stability. These extensions could improve the robustness and scalability of dynamic model routing, and enable more fine-grained control over the latency-accuracy trade-off.
      </p>
    </div>
  </div>
</section>
-->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{chen2025r,
        title={R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning},
        author={Chen, Zhuokun and Chen, Zeren and He, Jiahao and Tan, Mingkui and Cai, Jianfei and Zhuang, Bohan},
        journal={arXiv preprint arXiv:2507.17307},
        year={2025}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
