<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning</title>
  <link rel="icon" type="image/x-icon" href="static/images/efficiency.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>





<!-- Image carousel -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="mailto:caesard216@gmail.com" target="_blank">Zhuokun Chen</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Zeren Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Jiahao He</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Lu Sheng</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Mingkui Tan</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="#" target="_blank">Jianfei Cai</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="mailto:bohan.zhuang@gmail.com" target="_blank">Bohan Zhuang</a><sup>4‚Ä†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup> Monash University &nbsp;&nbsp;
              <sup>2</sup> School of Software, Beihang University &nbsp;&nbsp;
              <sup>3</sup> South China University of Technology &nbsp;&nbsp;
              <sup>4</sup> ZIP Lab, Zhejiang University
            </span>
            <span class="eql-cntrb"><small>Email: caesard216@gmail.com, bohan.zhuang@gmail.com</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2507.17307" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/Caesarhhh/R_Stitch" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              
              <span class="link-block">
                <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:L-aXZ6FU0VYJ:scholar.google.com/&output=citation&scisdr=CgIBXavQENivsT6gox0:AAZF9b8AAAAAaIWmux1fNZ6UnwDxaQkLs1214k4&scisig=AAZF9b8AAAAAaIWmuxkLjAKSxtpVwuQin0mfxMw&scisf=4&ct=citation&cd=-1&hl=zh-CN" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span>BibTex</span>
                </a>
              </span>
              <!-- ArXiv abstract Link -->
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<div class="content has-text-centered" style="font-size: 1.1rem;">
  <p>
    <strong>News:</strong><br>
    <span style="font-weight: bold;">(07/24/2025)</span> üéâ First version of the project is released on 
    <a href="https://arxiv.org/abs/2507.17307" target="_blank">arXiv</a>.
  </p>
</div>



<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Chain-of-thought (CoT) enhances the problem-solving ability of large language models (LLMs) but incurs substantial inference cost due to long autoregressive trajectories. Existing acceleration strategies either shorten traces via early stopping or compression, or adopt speculative decoding with a smaller model. However, speculative decoding provides limited gains when model agreement is low and rigidly enforces token-level consistency, overlooking the observation that some smaller models, when correct, produce significantly more concise reasoning traces that could reduce inference length. We introduce <em>R-Stitch</em>, a training-free hybrid decoding framework that leverages token-level entropy as an uncertainty proxy to delegate computation between a small language model (SLM) and an LLM. Our analysis shows that high-entropy tokens are more likely to induce errors, motivating an entropy-guided routing strategy that lets the SLM efficiently handle low-entropy tokens while delegating uncertain ones to the LLM, thereby avoiding full rollbacks and preserving answer quality. We further extend this design with <em>R-Stitch<sup>+</sup></em>, which learns an adaptive routing policy to adjust the token budget dynamically beyond fixed thresholds. By jointly reducing per-token decoding complexity and the number of generated tokens, our method achieves substantial acceleration with negligible accuracy loss. Concretely, it attains peak speedups of <b>3.00√ó</b> on DeepSeek-R1-Distill-Qwen-7B, <b>3.85√ó</b> on 14B, and <b>4.10√ó</b> on QWQ-32B while maintaining accuracy comparable to full LLM decoding. Moreover, it naturally enables adaptive efficiency‚Äìaccuracy trade-offs that can be tailored to diverse computational budgets without retraining. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<style>
  .section-title-box {
    font-size: 1.8rem;
    font-weight: 700;
    text-align: center;
    display: inline-block;
    padding: 0.4rem 1.2rem;
    border: 2px solid #333;
    border-radius: 8px;
    background: #fafafa;
  }
</style>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="section-title-box">
  Token-level Consistency and Speedup Analysis
  </h2>

    <!-- Intro text -->
    <div class="content has-text-justified" style="font-size: 1rem;">
      <p>
        Speculative decoding has received considerable attention due to its potential for substantial speedups. However, its effectiveness critically depends on the <b>consistency between the small language model (SLM) and the large language model (LLM)</b>. We quantify this limitation using <i>token-level consistency</i>, defined as the percentage of tokens for which the SLM produces the same output as the LLM given an identical prefix.
      </p>
      <p>
        Figure&nbsp;1 illustrates three aspects: (a) consistency‚Äìspeedup trade-offs across different model pairs, (b) distribution of speedups across AMC samples, and (c) token length comparison for correctly answered questions. The results highlight that speculative decoding may suffer overheads when agreement is low, and fails to exploit the concise reasoning of SLMs effectively.
      </p>
    </div>

    <!-- Coverflow Slider -->
    <div class="slider-wrapper" style="position: relative; width: 100%; max-width: 900px; margin: 2rem auto; overflow: hidden;">
      <div class="slider" style="position: relative; height: 500px;">

        <!-- Slide 1 -->
        <div class="slide">
          <img src="static/images/motivation_consistency_with_upperbound.png" alt="Token-level consistency vs. speedup">
          <p><b>(a)</b> Token-level consistency vs. speedup across LLMs.</p>
        </div>

        <!-- Slide 2 -->
        <div class="slide">
          <img src="static/images/speedup_distribution_filtered.png" alt="Speedup distribution">
          <p><b>(b)</b> Speedup distribution across AMC samples.</p>
        </div>

        <!-- Slide 3 -->
        <div class="slide">
          <img src="static/images/token_length_bars.png" alt="Token usage comparison">
          <p><b>(c)</b> Token usage by SLM vs. LLM on correct answers.</p>
        </div>

      </div>

      <!-- Navigation arrows -->
      <button onclick="prevSlide()" style="position: absolute; top: 50%; left: 10px; transform: translateY(-50%); background: rgba(255,255,255,0.9); border: none; border-radius: 50%; width: 36px; height: 36px; cursor: pointer; box-shadow: 0 2px 6px rgba(0,0,0,0.2); z-index: 10;">
        ‚óÄ
      </button>
      <button onclick="nextSlide()" style="position: absolute; top: 50%; right: 10px; transform: translateY(-50%); background: rgba(255,255,255,0.9); border: none; border-radius: 50%; width: 36px; height: 36px; cursor: pointer; box-shadow: 0 2px 6px rgba(0,0,0,0.2); z-index: 10;">
        ‚ñ∂
      </button>
    </div>

    <!-- Caption -->
    <div class="has-text-centered" style="margin-top: 1.5rem; font-size: 0.95rem; color: #555;">
      <b>Figure 1.</b> Token-level consistency and speedup analysis.  
      (a) Consistency vs. speedup across model pairs.  
      (b) Distribution of speedup ratios across AMC samples.  
      (c) Token usage comparison between SLM and LLM on correct answers.
    </div>
  </div>
</section>

<style>
  .slide {
    position: absolute;
    top: 50%;
    left: 50%;
    width: 70%;
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: flex-start;
    transform: translate(-50%, -50%) scale(0.7);
    text-align: center;
    transition: all 0.6s ease;
    opacity: 0;
    z-index: 0;
  }
  .slide img {
    max-width: 80%; /* Áº©Â∞èÂõæÁâá */
    height: auto;
    border-radius: 12px;
    box-shadow: 0 4px 12px rgba(0,0,0,0.15);
  }
  .slide p {
    margin-top: 0.6rem;
    font-size: 0.9rem;
    background: rgba(255,255,255,0.8); /* Èò≤Ê≠¢Ë¢´ÂõæÂÉèËæπÁºòÊå°‰Ωè */
    padding: 0.2rem 0.4rem;
    border-radius: 6px;
    display: inline-block;
  }
  .slide.active {
    opacity: 1;
    transform: translate(-50%, -50%) scale(1);
    z-index: 3;
  }
  .slide.left {
    opacity: 0.6;
    transform: translate(-125%, -50%) scale(0.85); /* ‰ªé -150% ÊîπÂà∞ -100% */
    z-index: 2;
  }
  .slide.right {
    opacity: 0.6;
    transform: translate(25%, -50%) scale(0.85); /* ‰ªé 50% ÊîπÂà∞ 0% */
    z-index: 2;
  }

</style>


<script>
  window.addEventListener("DOMContentLoaded", () => {
    let currentIndex = 0;
    const slides = document.querySelectorAll('.slide');

    function updateSlides() {
      slides.forEach((slide, i) => {
        slide.classList.remove("active", "left", "right");
        if (i === currentIndex) {
          slide.classList.add("active");
        } else if (i === (currentIndex - 1 + slides.length) % slides.length) {
          slide.classList.add("left");
        } else if (i === (currentIndex + 1) % slides.length) {
          slide.classList.add("right");
        }
      });
    }

    window.prevSlide = function () {
      currentIndex = (currentIndex - 1 + slides.length) % slides.length;
      updateSlides();
    }

    window.nextSlide = function () {
      currentIndex = (currentIndex + 1) % slides.length;
      updateSlides();
    }

    // ÂàùÂßãÂåñ
    updateSlides();
  });
</script>




<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered" style="margin-bottom: 1.5rem;">
      Empirical Entropy Analysis
    </h2>

    <!-- Intro text -->
    <div class="content has-text-justified" style="font-size: 1rem;">
      <p>
        Large language models (LLMs) exhibit stronger reasoning capabilities but incur substantially higher inference costs compared to small language models (SLMs). Prior work shows that when the two models produce consistent outputs, delegating computation to the SLM can effectively reduce latency. This raises a fundamental question: <b>under what conditions can SLM predictions be relied upon without compromising correctness?</b>
      </p>
      <p>
        To answer this, we conduct sample- and token-level entropy analyses on AMC using DeepSeek-R1-Distill-Qwen-7B (LLM) and L1-1.5B-Short (SLM), which reveal systematic patterns of entropy.
      </p>
      <ul>
        <li><b>(1)</b> Incorrect answers are associated with <i>higher entropy</i>.</li>
        <li><b>(2)</b> Most tokens have <i>extremely low entropy</i>, leaving only a small high-entropy fraction.</li>
        <li><b>(3)</b> High-entropy tokens are more likely to <i>trigger errors</i>.</li>
      </ul>
    </div>

    <!-- Coverflow Slider -->
    <div class="slider-wrapper" style="position: relative; width: 100%; max-width: 900px; margin: 2rem auto; overflow: hidden;">
      <div class="slider" style="position: relative; height: 500px;">

        <!-- Slide 1 -->
        <div class="slide">
          <img src="static/images/motivation/entropy_amc.png" alt="Sample-level entropy in correct vs incorrect solutions">
          <p><b>(a)</b> Incorrect solutions exhibit higher entropy than correct ones.</p>
        </div>

        <!-- Slide 2 -->
        <div class="slide">
          <img src="static/images/motivation/token_entropys.png" alt="Token-level entropy distribution">
          <p><b>(b)</b> The entropy distribution is heavily skewed toward zero; most tokens have (near-)zero entropy.</p>
        </div>

        <!-- Slide 3 -->
        <div class="slide">
          <img src="static/images/motivation/entropy_neighborhood.png" alt="Entropy around harmful tokens">
          <p><b>(c)</b> Context around harmful tokens shows elevated entropy, signaling error-prone regions.</p>
        </div>

      </div>

      <!-- Navigation arrows -->
      <button onclick="prevSlide2()" style="position: absolute; top: 50%; left: 10px; transform: translateY(-50%); background: rgba(255,255,255,0.9); border: none; border-radius: 50%; width: 36px; height: 36px; cursor: pointer; box-shadow: 0 2px 6px rgba(0,0,0,0.2); z-index: 10;">
        ‚óÄ
      </button>
      <button onclick="nextSlide2()" style="position: absolute; top: 50%; right: 10px; transform: translateY(-50%); background: rgba(255,255,255,0.9); border: none; border-radius: 50%; width: 36px; height: 36px; cursor: pointer; box-shadow: 0 2px 6px rgba(0,0,0,0.2); z-index: 10;">
        ‚ñ∂
      </button>
    </div>

    <!-- Caption -->
    <div class="has-text-centered" style="margin-top: 1.5rem; font-size: 0.95rem; color: #555;">
      <b>Figure 2.</b> Entropy and error locality.  
      (a) Incorrect solutions exhibit higher entropy.  
      (b) Distribution is skewed toward zero; most tokens have near-zero entropy.  
      (c) Harmful token neighborhoods show elevated entropy.
    </div>
  </div>
</section>

<style>
  .slide {
    position: absolute;
    top: 50%;
    left: 50%;
    width: 70%;
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: flex-start;
    transform: translate(-50%, -50%) scale(0.7);
    text-align: center;
    transition: all 0.6s ease;
    opacity: 0;
    z-index: 0;
  }
  .slide img {
    max-width: 80%; /* ËøôÈáåÊ≤øÁî®‰Ω†ÂñúÊ¨¢ÁöÑ 80% */
    height: auto;
    border-radius: 12px;
    box-shadow: 0 4px 12px rgba(0,0,0,0.15);
  }
  .slide p {
    margin-top: 0.6rem;
    font-size: 0.9rem;
    background: rgba(255,255,255,0.8);
    padding: 0.2rem 0.4rem;
    border-radius: 6px;
    display: inline-block;
  }
  .slide.active {
    opacity: 1;
    transform: translate(-50%, -50%) scale(1);
    z-index: 3;
  }
  .slide.left {
    opacity: 0.6;
    transform: translate(-125%, -50%) scale(0.85); /* ‰Ω†ÂÆöÁöÑÊúÄ‰Ω≥ÂèÇÊï∞ */
    z-index: 2;
  }
  .slide.right {
    opacity: 0.6;
    transform: translate(25%, -50%) scale(0.85); /* ‰Ω†ÂÆöÁöÑÊúÄ‰Ω≥ÂèÇÊï∞ */
    z-index: 2;
  }
</style>

<script>
  window.addEventListener("DOMContentLoaded", () => {
    let currentIndex2 = 0;
    const slides2 = document.querySelectorAll('.slider-wrapper:nth-of-type(1) .slide');

    function updateSlides2() {
      slides2.forEach((slide, i) => {
        slide.classList.remove("active", "left", "right");
        if (i === currentIndex2) {
          slide.classList.add("active");
        } else if (i === (currentIndex2 - 1 + slides2.length) % slides2.length) {
          slide.classList.add("left");
        } else if (i === (currentIndex2 + 1) % slides2.length) {
          slide.classList.add("right");
        }
      });
    }

    window.prevSlide2 = function () {
      currentIndex2 = (currentIndex2 - 1 + slides2.length) % slides2.length;
      updateSlides2();
    }

    window.nextSlide2 = function () {
      currentIndex2 = (currentIndex2 + 1) % slides2.length;
      updateSlides2();
    }

    updateSlides2();
  });
</script>






<!-- Video carousel -->
<section class="hero is-small is-light" >
  <div class="hero-body">
    <div class="container" style="max-width: 70%; margin: 0 auto;">
      <h2 class="title is-3">Methodology</h2>
      <div class="has-text-centered">
      <figure class="image is-inline-block">
        <img src="static/images/cot_methodv2.png" alt="Method illustration"  style="width: 95%;">
      </figure>
      <figcaption>
        <b>Overview of R-Stitch.</b> Given a question with chain-of-thought (CoT) prompting, decoding alternates between a small language model (SLM) and a large language model (LLM) based on token-level confidence. Generation begins with the SLM. If the predicted token has low confidence, the system switches to the LLM, which overwrites the uncertain token and continues decoding. Conversely, when the LLM produces a high-confidence token, control is handed back to the SLM to reduce computational cost. This bidirectional switching mechanism enables dynamic resource allocation based on confidence, allowing R-Stitch~to retain the efficiency of the SLM while leveraging the reliability of the LLM when necessary.
      </figcaption>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="max-width: 70%; margin: 0 auto;">
      <h2 class="title is-3">Performance</h2>

      <div class="has-text-centered">
        <figure class="image is-inline-block">
          <img src="static/images/performance.png" alt="Performance">
        </figure>
      </div>

    </div>
  </div>
</section>


<!--
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container content" style="max-width: 70%; margin: 0 auto;">
      <h2 class="title is-3">Limitations and Future Work</h2>

      <p>
        The current routing policy is based on a fixed confidence threshold, which may lead to unstable performance across diverse input distributions and task types. In particular, confidence scores may not always accurately reflect token difficulty, resulting in suboptimal routing decisions. Additionally, our implementation <strong>currently supports only batch size 1</strong> due to dynamic token-level model switching, which limits practical deployment and hardware utilization. Addressing this limitation may require designing new scheduling strategies or restructuring the routing mechanism to better accommodate batched inference.
      </p>

      <p>
        To further alleviate the KV Cache and parameter burden from the two models, we plan to adopt <em>parameter-sharing strategies</em> such as mixture-of-depth/width to enhance memory efficiency. As a future direction, we also plan to explore <strong>reinforcement learning-based policies</strong> that can adaptively switch models based on feedback signals like downstream task performance or decoding stability. These extensions could improve the robustness and scalability of dynamic model routing, and enable more fine-grained control over the latency-accuracy trade-off.
      </p>
    </div>
  </div>
</section>
-->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{chen2025r,
        title={R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning},
        author={Chen, Zhuokun and Chen, Zeren and He, Jiahao and Tan, Mingkui and Cai, Jianfei and Zhuang, Bohan},
        journal={arXiv preprint arXiv:2507.17307},
        year={2025}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
